{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Document Classification (multiclass outcome).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNhL3Dc5x986"
      },
      "source": [
        "#Document Classification using ML models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOuhbg-XG312"
      },
      "source": [
        "We will use a dataset on 2225 BBC news articles for this activity. The article text and its category are given in our dataset. \n",
        "Upload the \"bbc-news.csv\" dataset (download it from elearn) to your session before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRbey1H5wVQN"
      },
      "source": [
        "#import the dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv('bbc-news.csv')\n",
        "df['category_id'] = df.category.factorize()[0] #we need this later\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY67-xFOI-fE"
      },
      "source": [
        "# frequency of articles in each category \n",
        "import seaborn as sb\n",
        "sb.countplot(data=df, x='category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsc8TYtw_dPO"
      },
      "source": [
        "## Text pre-processing\n",
        "\n",
        "*TfidfVectorizer* lower-cases all words, removes punctuations and stopwords, tokenizes the documents, and derives the tf-idf document-term matrix. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDKrAAeNw2IO"
      },
      "source": [
        "X=df.text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# only keep words that have min frequency=5 in our corpus (min_df=4)\n",
        "tfidf = TfidfVectorizer(min_df=4, encoding='latin-1', ngram_range=(1, 1), stop_words='english')\n",
        "X_tfidf = tfidf.fit_transform(df.text).toarray()\n",
        "Y = df.category_id\n",
        "X_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TydZBcjzVbjz"
      },
      "source": [
        "# Let's take a look at an article and its tokens from the tf-idf matrix\n",
        "print(df.text[2]),\n",
        "tfidf.inverse_transform(X_tfidf[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkVycVCK7yXf"
      },
      "source": [
        "# number of tokens in each document (review) of our corpus\n",
        "num_tokens = [np.count_nonzero(tokens) for tokens in X_tfidf]\n",
        "max(num_tokens) #longest document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q339lNipT29"
      },
      "source": [
        "# number of words in our vocabulary\n",
        "num_words=len(tfidf.vocabulary_)\n",
        "num_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiovQh8Ii1ja"
      },
      "source": [
        "## Split the data into test/train\n",
        "We split the tranformed data (i.e, the tf-idf doc-term matrix) into testing and training set. I keep a small part for the testing set here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSuOFzlUi1Ah"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.20,random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEf8fk0eIJx2"
      },
      "source": [
        "## ML models for text classification\n",
        "We will use Logistic Regression, multinomial Naive Bayes, and SVM for this multiclass classification problem.\n",
        "We train all models using k-fold Cross validation. \n",
        "\n",
        "For more details on *Multinomial Naive Bayes*, see https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes and \n",
        "https://stats.stackexchange.com/questions/33185/difference-between-naive-bayes-multinomial-naive-bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzyv9GiZKZ3L"
      },
      "source": [
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('LogisticReg', LogisticRegression()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('MultinomialNB', MultinomialNB()))\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy' #metric we want to compare\n",
        "\n",
        "for name, model in models:\n",
        "\tkfold = model_selection.KFold(n_splits=5,shuffle=True, random_state=0)\n",
        "\tcv_results = model_selection.cross_val_score(model, X_tfidf, Y, cv=kfold, scoring=scoring)\n",
        "  #cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "\tresults.append(cv_results)\n",
        "\tnames.append(name)\n",
        "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "\tprint(msg)\n",
        "# boxplot model comparison\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Comparing Text classification models: ' +scoring)\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YULdg3n8NbX"
      },
      "source": [
        "### Question 1\n",
        "Which model has a better accuracy in predicting the article's category?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2A8US_2cUU0"
      },
      "source": [
        "## Evaluate Logistic Regression classifier's performance \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZl_wIcaGxx"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "# Logistic regression\n",
        "model_LogReg = LogisticRegression(random_state=0)\n",
        "model_LogReg.fit(X_train, y_train)\n",
        "\n",
        "#y_pred_proba = model_LogReg.predict_proba(X_test)\n",
        "y_pred = model_LogReg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiA24fE5fXgW"
      },
      "source": [
        "!pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCHL_7-7ZnLZ"
      },
      "source": [
        "import scikitplot as skplt\n",
        "skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)), #Accuracy\n",
        "print(\"Balanced Acc.:\",metrics.balanced_accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmRqXZAXewwo"
      },
      "source": [
        "From earlier we know: \n",
        "tech=0, \n",
        "business=1, \n",
        "sport=2, \n",
        "entertainment=3, \n",
        "politics=4\n",
        "\n",
        "Seems a Logistic Regression does a pretty good job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivf0YsUO7EDs"
      },
      "source": [
        "## Label a new article using the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-4sPSyh6CcF"
      },
      "source": [
        "# label a new review using our model\n",
        "new_article= ['nobody knows where he is or what he does'] \n",
        "#process new review into doc-term matrix \n",
        "X_new=tfidf.transform(new_article) \n",
        "\n",
        "print(model_LogReg.predict_proba(X_new)) #predict class probabilities \n",
        "print(model_LogReg.predict(X_new)) #predict outcome class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_NkzkZ8EOD"
      },
      "source": [
        "### Question 2\n",
        "Which category does the logistic regression assign this new article to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm2rUUlsKOPl"
      },
      "source": [
        "# Document Classification using word embeddings and RNNs (LSTM/GRU)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S2vPhiTjhjw"
      },
      "source": [
        "import tensorflow.keras as tk\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, GRU, Bidirectional,RNN\n",
        "tk.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJrxMa5DQrv0"
      },
      "source": [
        "## Preprocessing text data for RNN \n",
        "To use Recurrent neural network architectures for text classification, we need to pre-process text data little differently from what we did above.\n",
        "* We create a vocabulary by tokenizing all document in our corpus. \n",
        "* Then, we sequence each document into integer-tokens using the derived vocabulary. \n",
        "* To correctly feed each document input to the RNN, each sequenced document has to have the same length. Therefore, we pad/truncate documents. \n",
        "* We use an embedding layer to convert the integer-vectors into real-valued vectors (more details in section below)\n",
        "* The output of the embedding layer is fed into our Recurrent NN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JdGvtbuo5U8"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer #removes punctuations and lower-cases all words before tokenizing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EIOW-sFULB5"
      },
      "source": [
        "### Tokenizing, sequencing, padding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2WssYKXf-71"
      },
      "source": [
        "num_words=10000 #number of words to keep in vocabulary, most common words are kept\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6tSqF4vcflR"
      },
      "source": [
        "len(tokenizer.word_index) # total number of unique words in our corpus\n",
        "#tokenizer.word_index # run to see what integer is assigned to each word after tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5LREy5ghXB8"
      },
      "source": [
        "# sequencing our text data (tokenizing based)\n",
        "X_tokens=tokenizer.texts_to_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhkIerJ3h9GH"
      },
      "source": [
        "# some values we need for the next step\n",
        "num_tokens= np.array([np.count_nonzero(tokens) for tokens in X_tokens]) #number of tokens in each document\n",
        "max_tokens=max(num_tokens)\n",
        "max_tokens,np.mean(num_tokens),np.std(num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsoypBbCipok"
      },
      "source": [
        "# pad the token sequence for each document so that all documents are of equal length\n",
        "max_tokens=1000\n",
        "X_tokens_pad = pad_sequences(X_tokens, maxlen=max_tokens,\n",
        "                            padding='pre', truncating='pre')\n",
        "X_tokens_pad.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hIEeJqijwpr"
      },
      "source": [
        "# let's see what happened to a single document at each step\n",
        "print(X[17]) #the textual data\n",
        "print(X_tokens[17]) #the tokenized sequence\n",
        "print(X_tokens_pad[17]) #the padded sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hubATLa7L8-m"
      },
      "source": [
        "## Splitting the processed data into train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znkaFH4Hy6hR"
      },
      "source": [
        "# split processed data into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tokens_pad, Y, test_size=0.2,random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pEI0aU1aOl"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5QFsyzL97u"
      },
      "source": [
        "## Building a Recurrent neural network \n",
        "The first layer is an embedding layer that converts the integer-tokens into vector values. \n",
        "This is needed since the integer-tokens (X_tokens) can have values between 0 and 1800 (for our vocabulary of 1800 words), but the RNN cannot work with such wide range. The learned vector values range from -1 to 1.\n",
        "\n",
        "The embedding-layer is trained as part of the RNN and learns to map words with similar meaning to similar embedding vectors. We will try an example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tziRcOnUhGxt"
      },
      "source": [
        "embedding_size=100 #size of the embedding vector\n",
        "\n",
        "model_NN = tk.models.Sequential()\n",
        "\n",
        "model_NN.add(Embedding(input_dim=num_words,\n",
        "                    output_dim=embedding_size,\n",
        "                    input_length=max_tokens,\n",
        "                    name='layer_embedding'))\n",
        "\n",
        "#model_NN.add(LSTM(units=32,return_sequences=True,dropout=0.2)) # add an LSTM layer\n",
        "#model_NN.add(Dropout(0.2))\n",
        "\n",
        "model_NN.add(LSTM(units=16,dropout=0.2)) # add last LSTM layer\n",
        "#model_NN.add(GRU(units=16,dropout=0.2)) #add last GRU layer\n",
        "\n",
        "# for sequences other than time series, an RNN model can sometimes perform better if it processes a sequence both forwards and backwards. \n",
        "# For example, to predict the next word in a sentence it is useful to have the context around the word.\n",
        "# the Bidirectional wrapper adds such layers.\n",
        "#model_NN.add(Bidirectional(LSTM(units=16,dropout=0.2)))\n",
        "\n",
        "model_NN.add(Dropout(0.2))\n",
        "\n",
        "# the output layers has 5 units since we have 5 outcome categories\n",
        "# we use the softmax function to get a probability will for each outcome category\n",
        "model_NN.add(Dense(5, activation='softmax'))\n",
        "\n",
        "optimizer=tk.optimizers.Adam() # selecting the optimizer\n",
        "\n",
        "# compiling the model\n",
        "model_NN.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "model_NN.summary() # model overview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqIdUEMOguSc"
      },
      "source": [
        "# make sure the input data can be feed to the NN\n",
        "model_NN.predict(X_train[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLdwnUXuir5Q"
      },
      "source": [
        "# train the RNN model (NOTE! it can take a while!)\n",
        "history = model_NN.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=30, verbose=1)\n",
        "#history = model_NN.fit(X_train, y_train,validation_split=0.2,epochs=10, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHd2104wBZ1r"
      },
      "source": [
        "# plot training and validation accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'],label=\"training\")\n",
        "plt.plot(history.history['val_accuracy'],label=\"validation\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTf7dL-_O4X"
      },
      "source": [
        "# plot training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'],label=\"training\")\n",
        "plt.plot(history.history['val_loss'],label=\"validation\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYBeoAXk64C"
      },
      "source": [
        "### Question 3\n",
        "Do we need to run the model for more epochs? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HTVJB1aDVKA"
      },
      "source": [
        "## Evaluate  RNN model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs1i2DLRCzTQ"
      },
      "source": [
        "y_pred=np.argmax(model_NN.predict(X_test),axis=-1)\n",
        "\n",
        "import scikitplot as skplt\n",
        "skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)), #Accuracy\n",
        "print(\"Balanced Acc.:\",metrics.balanced_accuracy_score(y_test, y_pred))\n",
        "#tech=0\n",
        "#business=1\n",
        "#sport=2\n",
        "#entertainment=3\n",
        "#politics=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQQXjQ_wnCnD"
      },
      "source": [
        "### Question 4\n",
        "Compare the RNN model with the Logistic Regression Classifier in terms of accuracy. Does the RNN model perform better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obcJUde6ai2G"
      },
      "source": [
        "## Classify a new input/article using the RNN \n",
        "Let's see how our classifier performs on a new article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E5KDBSvao0W"
      },
      "source": [
        "new_review= ['nobody knows who he is or what he does'] \n",
        "new_sequence=tokenizer.texts_to_sequences(new_review) # tokenize and sequence\n",
        "new_seq_pad=pad_sequences(new_sequence, maxlen=max_tokens,padding='pre') # pad it\n",
        "new_seq_pad;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPw4g6CebPQj"
      },
      "source": [
        "model_NN.predict(new_seq_pad) # predict probability \n",
        "#np.argmax(model_NN.predict(X),axis=-1) #for multiclass outcomes\n",
        "#(model_NN.predict(new_seq_pad)>0.5).astype('int32') #for binary outcomes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geDg_IyzLB1A"
      },
      "source": [
        "# let's see \n",
        "new_article= ['In a recent study led by the University of Bristol, scientists have shown how to simultaneously \\\n",
        "harness multiple forms of regulation in living cells to strictly control gene expression and open new avenues \\\n",
        "for improved biotechnologies. Engineered microbes are increasingly being used to enable the sustainable and \\\n",
        "clean production of chemicals, medicines and much more. To make this possible, bioengineers must control when \\\n",
        "specific sets of genes are turned on and off to allow for careful regulation of the biochemical processes involved.'] \n",
        "new_sequence=tokenizer.texts_to_sequences(new_article) # tokenize and sequence\n",
        "new_seq_pad=pad_sequences(new_sequence, maxlen=max_tokens,padding='pre') # pad it\n",
        "new_seq_pad;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXt1fU_yLNV1"
      },
      "source": [
        "model_NN.predict(new_seq_pad) # predict probability "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feaps9L3kEBt"
      },
      "source": [
        "### Question 5\n",
        "Which category does the RNN model assign this new article to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwohWWkYJIEG"
      },
      "source": [
        "## Extract word embeddings\n",
        "Earlier we mentioned word embeddings and that we have an embedding layer which learns how to map words (in the form of integer-tokens) to vectors as we train the neural network. \n",
        "\n",
        "The distance between two word vectors indicate their similarity and we can derive these vectors for any word in our vocabulary from the embedding layer. \n",
        "\n",
        "We will use the cosine distance between two words. Cosine values can range from [-1,1], so the distance can range frmo 0 to 2 (in our complex vector space): \n",
        "* cosine distance close to 0: two words have the same meaning \n",
        "* cosine distance close to 2: two words are antonyms\n",
        "\n",
        "For more details on cosine similarity, see https://en.wikipedia.org/wiki/Cosine_similarity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMH4ojV8SS_7"
      },
      "source": [
        "# extract the embedding layer from the RNN architecture\n",
        "embedding_weights=model_NN.get_layer('layer_embedding').get_weights()[0]\n",
        "\n",
        "# vector for a word in our vocabulary\n",
        "print(tokenizer.word_index['service']) # integer for word in our vocabulary\n",
        "# word vector representation as learned in the embedding layer\n",
        "embedding_weights[tokenizer.word_index['service']]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik9DQ0fzeT6D"
      },
      "source": [
        "# let's calculate distance between any two words in our vocabulary\n",
        "from scipy.spatial.distance import cdist\n",
        "float(cdist([embedding_weights[tokenizer.word_index['tree']]],\n",
        "            [embedding_weights[tokenizer.word_index['sky']]],\n",
        "            metric='cosine'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}